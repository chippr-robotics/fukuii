fukuii {
  # Optionally augment the client ID sent in Hello messages.
  client-identity = null

  # Base directory where all the data used by the node is stored, including blockchain data and private keys
  datadir = ${user.home}"/.fukuii/"${fukuii.blockchains.network}

  # The unencrypted private key of this node
  node-key-file = ${fukuii.datadir}"/node.key"

  # timeout for shutting down the ActorSystem
  shutdown-timeout = "15.seconds"

  # Whether to run Fukuii in test mode (similar to --test flag in cpp-ethereum).
  # When set, test validators and consensus are used by this node.
  # It also enables test_ RPC endpoints.
  testmode = false

  # one of the algorithms defined here:
  # https://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html#SecureRandom
  # Uncomment this to specify, otherwise use the default implementation
  # secure-random-algo = "NativePRNG"

  keyStore {
    # Keystore directory: stores encrypted private keys of accounts managed by this node
    keystore-dir = ${fukuii.datadir}"/keystore"

    # Enforces minimal length for passphrase of this keystore
    minimal-passphrase-length = 7

    # Allows possibility for no passphrase
    # If passphrase is set it must be greater than minimal-passphrase-length
    allow-no-passphrase = true
  }

  network {
    server-address {
      # Listening interface for Ethereum protocol connections
      interface = "0.0.0.0"

    }

    # Try automatic port forwarding via UPnP
    automatic-port-forwarding = true

    discovery {

      # Turn discovery of/off
      discovery-enabled = true

      # Externally visible hostname or IP.
      host = null

      # Listening interface for discovery protocol
      interface = "0.0.0.0"

      # Listening port for discovery protocol
      port = 30303

      # If true, the node considers the bootstrap and the previously persisted nodes
      # as already discovered and uses them as peer candidates to get blocks from.
      # Otherwise it enroll with the bootstrap nodes and gradually discover the
      # network every time we start, eventually serving candidates.
      #
      # Useful if discovery has problem, as the node can start syncing with the
      # bootstraps straight away.
      #
      # Note that setting reuse-known-nodes and discovery-enabled to false at the
      # same time would mean the node would have no peer candidates at all.
      reuse-known-nodes = true

      # Scan interval for discovery
      # Increased from 1 minute to 2 minutes to reduce network overhead and peer churn
      scan-interval = 2.minutes

      # Discovery message expiration time
      message-expiration = 1.minute

      # Maximum amount a message can be expired by,
      # accounting for possible discrepancies between nodes' clocks.
      max-clock-drift = 15.seconds

      # Maximum number of peers in each k-bucket.
      kademlia-bucket-size = 16

      # Timeout for individual requests like Ping.
      # Increased from 1s to 3s to allow for network latency and prevent premature timeouts
      request-timeout = 3.seconds

      # Timeout to collect all possible responses for a FindNode request.
      # Increased from 5s to 10s to allow more time for responses on high-latency networks
      kademlia-timeout = 10.seconds

      # Level of concurrency during lookups and enrollment.
      kademlia-alpha = 3

      # Maximum number of messages in the queue associated with a UDP channel.
      channel-capacity = 100
    }

    known-nodes {
      # How often known nodes updates are persisted to disk
      persist-interval = 20.seconds

      # Maximum number of persisted nodes
      max-persisted-nodes = 200
    }

    peer {
      # DevP2P protocol version advertised in Hello handshake.
      # Compression is only expected when this is >= 5.
      # Override per environment to force compression off (set to 4) during investigations.
      p2p-version = 5

      # Retry delay for failed attempt at connecting to a peer
      # Increased from 5s to 15s to reduce connection churn and give network conditions time to stabilize
      connect-retry-delay = 15 seconds

      # Maximum number of reconnect attempts after the connection has been initiated.
      # After that, the connection will be dropped until its initiated again (eg. by peer discovery)
      # Reduced from 3 to 2 to fail faster and try new peers instead of retrying failing connections
      connect-max-retries = 2

      disconnect-poison-pill-timeout = 15 seconds

      wait-for-hello-timeout = 5 seconds

      wait-for-status-timeout = 30 seconds

      wait-for-chain-check-timeout = 15 seconds

      # Increased from 6s to 10s to allow for slower handshakes on high-latency networks
      wait-for-handshake-timeout = 10 seconds

      # Increased from 10s to 15s to prevent premature TCP write failures
      wait-for-tcp-ack-timeout = 15 seconds

      # Maximum block headers in a single response message (as a blockchain host)
      # ETH protocol uses 2MB soft response limit (core-geth)
      # Block headers are ~500-600 bytes each; pushed to safe maximum
      # 1024 headers ≈ 512-614KB, well under 2MB limit with significant margin
      max-blocks-headers-per-message = 1024

      # Maximum block bodies in a single response message (as a blockchain host)
      # Block bodies vary widely (empty blocks to full blocks with many txs)
      # Average block ~20-50KB, setting to 50 blocks ≈ 1-2.5MB under typical conditions
      # Reduced from 100 to 50 to stay reliably under 2MB soft limit
      max-blocks-bodies-per-message = 50

      # Maximum transactions receipts in a single response message (as a blockchain host)
      # Receipts vary by log count: simple ~200-500 bytes, complex with logs ~1-5KB
      # 1024 receipts at 1KB average ≈ 1MB, safe maximum under 2MB limit
      max-receipts-per-message = 1024

      # Maximum MPT components in a single response message (as a blockchain host)
      max-mpt-components-per-message = 200

      # Minimum number of peers this node tries to connect to at all times
      min-outgoing-peers = 20

      # Maximum number of peers this node can connect to at any time.
      # It's a bit higher than max-incoming-peers so that the node can quickly churn through incompatible peers after startup.
      max-outgoing-peers = 50

      # Maximum number of peers that can connect to this node.
      # Should be at least as much as `min-outgoing-peers` so on a network level `total(max-in) >= total(min-out)`
      max-incoming-peers = 30

      # Number of incoming peers to prune if we hit the maximum, to free up slots for new connections.
      prune-incoming-peers = 10

      # Minimum age of peers before they can be selected for pruning, and the minimum time to pass between pruning attempts.
      min-prune-age = 30.minutes

      # Maximum number of peers that can be connecting to this node
      max-pending-peers = 200

      # Initial delay before connecting to nodes
      # Increased from 10s to 15s to allow discovery to populate more candidates before connecting
      update-nodes-initial-delay = 15.seconds

      # Newly discovered nodes connect attempt interval
      # Increased from 30s to 60s to reduce connection churn and AlreadyConnected disconnects
      update-nodes-interval = 60.seconds

      # Peer which disconnect during tcp connection because of too many peers will not be retried for this short duration
      # Reduced from 6 minutes to 3 minutes to allow faster retry when peer slots become available
      short-blacklist-duration = 3.minutes

      # Peer which disconnect during tcp connection because of other reasons will not be retried for this long duration
      # other reasons include: timeout during connection, wrong protocol, incompatible network
      # Reduced from 600 minutes (10 hours) to 60 minutes (1 hour) to allow reconnection to peers
      # that may have had transient issues
      long-blacklist-duration = 60.minutes

      # Resolution of moving window of peer statistics.
      # Will be multiplied by `stat-slot-count` to give the overall length of peer statistics availability.
      stat-slot-duration = 10.minutes

      # How many slots of peer statistics to keep in the moving window, e.g. 60 * 1.minute to keep stats for the last hour with 1 minute resolution.
      stat-slot-count = 72
    }

    rpc {
      http {
        # JSON-RPC mode
        # Available modes are: http, https
        # Choosing https requires creating a certificate and setting up 'certificate-keystore-path' and
        # 'certificate-password-file'
        # See: https://github.com/input-output-hk/fukuii/wiki/Creating-self-signed-certificate-for-using-JSON-RPC-with-HTTPS
        mode = "http"

        # Whether to enable JSON-RPC HTTP(S) endpoint
        enabled = true

        # Listening address of JSON-RPC HTTP(S) endpoint
        interface = "localhost"

        # Listening port of JSON-RPC HTTP(S) endpoint
        port = 8546

        certificate = null
        #certificate {
        # Path to the keystore storing the certificates (used only for https)
        # null value indicates HTTPS is not being used
        #  keystore-path = "tls/fukuiiCA.p12"

        # Type of certificate keystore being used
        # null value indicates HTTPS is not being used
        #  keystore-type = "pkcs12"

        # File with the password used for accessing the certificate keystore (used only for https)
        # null value indicates HTTPS is not being used
        #  password-file = "tls/password"
        #}

        # Domains allowed to query RPC endpoint. Use "*" to enable requests from
        # any domain.
        cors-allowed-origins = []

        # Rate Limit for JSON-RPC requests
        # Limits the amount of request the same ip can perform in a given amount of time
        rate-limit {
          # If enabled, restrictions are applied
          enabled = false

          # Time that should pass between requests
          min-request-interval = 10.seconds

          # Size of stored timestamps for requests made from each ip
          latest-timestamp-cache-size = 1024
        }
      }

      ipc {
        # Whether to enable JSON-RPC over IPC
        enabled = false

        # Path to IPC socket file
        socket-file = ${fukuii.datadir}"/fukuii.ipc"
      }

      # Enabled JSON-RPC APIs over the JSON-RPC endpoint
      # Available choices are: web3, eth, net, personal, fukuii, test, iele, debug, qa, checkpointing
      apis = "eth,web3,net,personal,fukuii,debug,qa,checkpointing"

      # Maximum number of blocks for fukuii_getAccountTransactions
      account-transactions-max-blocks = 1000

      net {
        peer-manager-timeout = 20.seconds
      }

      health {
        # If the best known block number stays the same for more time than this,
        # the healthcheck will consider the client to be stuck and return an error
        no-update-duration-threshold = 30.minutes
        # If the difference between the best stored block number and the best known block number
        # is less than this value, the healthcheck will report that the client is synced.
        syncing-status-threshold = 10
      }

      miner-active-timeout = 30.seconds
    }
  }

  txPool {
    # Maximum number of pending transaction kept in the pool
    tx-pool-size = 1000

    pending-tx-manager-query-timeout = 60.seconds

    transaction-timeout = 2.minutes

    # Used in mining (ethash)
    get-transaction-from-pool-timeout = 60.seconds
  }

  mining {
    # Miner's coinbase address
    # Also used in non-Ethash consensus.
    coinbase = "0011223344556677889900112233445566778899"

    # Extra data to add to mined blocks
    header-extra-data = "fukuii"

    # This determines how many parallel eth_getWork request we can handle, by storing the prepared blocks in a cache,
    # until a corresponding eth_submitWork request is received.
    #
    # Also used by the generic `BlockGenerator`.
    block-cashe-size = 30

    # See io.iohk.ethereum.consensus.mining.Protocol for the available protocols.
    # Declaring the protocol here means that a more protocol-specific configuration
    # is pulled from the corresponding consensus implementation.
    # For example, in case of ethash, a section named `ethash` is used.
    # Available protocols: pow, mocked, restricted-pow
    # In case of mocked, remember to enable qa api
    protocol = pow

    # If true then the consensus protocol uses this node for mining.
    # In the case of ethash PoW, this means mining new blocks, as specified by Ethereum.
    # In the general case, the semantics are due to the specific consensus implementation.
    mining-enabled = false
  }

  # This is the section dedicated to Ethash mining.
  # This consensus protocol is selected by setting `fukuii.consensus.protocol = ethash`.
  pow {
    # Maximum number of ommers kept in the pool
    ommers-pool-size = 30

    ommer-pool-query-timeout = 60.seconds

    ethash-dir = ${user.home}"/.ethash"

    mine-rounds = 100000
  }

  blockchains {
    network = "etc"

    # Optional: Directory containing custom chain configuration files
    # Custom chain configs should be named <network>-chain.conf (e.g., mynetwork-chain.conf)
    # and placed in this directory. Custom chains will override built-in chains with the same name.
    # To use: set this property via -Dfukuii.blockchains.custom-chains-dir=/path/to/chains
    # or in your custom config file
    # custom-chains-dir = "/path/to/custom/chains"

    etc {include required("chains/etc-chain.conf")}

    eth {include required("chains/eth-chain.conf")}

    mordor {include required("chains/mordor-chain.conf")}

    pottery	{include required("chains/pottery-chain.conf")}

    ropsten {include required("chains/ropsten-chain.conf")}

    test {include required("chains/test-chain.conf")}

    testnet-internal-nomad {include required("chains/testnet-internal-nomad-chain.conf")}

    gorgoroth {include required("chains/gorgoroth-chain.conf")}
  }

  sync {
    # Whether to enable fast-sync
    do-fast-sync = true

    # Whether to enable SNAP sync (takes priority over fast-sync if enabled)
    do-snap-sync = true

    # SNAP sync configuration
    # SNAP sync is a state synchronization protocol that downloads account and storage ranges
    # without intermediate Merkle trie nodes, significantly reducing sync time and bandwidth.
    # See: https://github.com/ethereum/devp2p/blob/master/caps/snap.md
    snap-sync {
      # Enable SNAP sync (requires do-snap-sync = true)
      enabled = true

      # Number of blocks before the best block to use as pivot for SNAP sync
      # Core-geth uses 64 blocks (fsMinFullBlocks) as the minimum threshold
      # This allows SNAP sync to start much sooner after genesis
      # A higher offset provides more stability against chain reorgs
      # A lower offset reduces the catch-up time after SNAP sync completes
      # Changed from 1024 to 64 to match core-geth exactly (for SNAP sync only)
      # Note: Fast-sync has its own separate pivot-block-offset setting (32 blocks)
      pivot-block-offset = 64

      # Number of concurrent account range download tasks
      # Higher values increase throughput but may overwhelm peers
      # Recommended: 16 tasks (divides 256-bit address space into 16 ranges)
      account-concurrency = 16

      # Number of concurrent storage range download tasks
      # Storage downloads are typically less voluminous than accounts
      # Recommended: 8 tasks (balances throughput with peer load)
      storage-concurrency = 8

      # Maximum number of accounts to request storage for in a single batch
      # Batching reduces message overhead
      # Recommended: 8 accounts per batch
      storage-batch-size = 8

      # Maximum number of trie node paths to request in a single healing request
      # Healing requests are typically fewer and can be larger
      # Recommended: 16 paths per batch
      healing-batch-size = 16

      # Whether to validate state completeness before transitioning to regular sync
      # When enabled, walks the entire state trie to detect missing nodes
      # Recommended: true for production (ensures correctness)
      # Can be disabled for testing/debugging
      state-validation-enabled = true

      # Maximum number of retries for failed SNAP sync requests
      # Failed requests are retried with exponential backoff
      # Recommended: 3 retries
      max-retries = 3

      # Timeout for SNAP sync requests (GetAccountRange, GetStorageRanges, etc.)
      # Should be long enough to allow large responses to be transmitted
      # Recommended: 30 seconds (matches peer-response-timeout)
      timeout = 30 seconds

      # Maximum number of critical SNAP sync failures before fallback to fast sync
      # Critical failures include circuit breaker trips and state validation failures
      # Recommended: 5 failures (provides enough retries while preventing infinite loops)
      max-snap-sync-failures = 5
    }

    # Interval for updating peers during sync
    # Increased from 3s to 5s to reduce peer scanning overhead
    peers-scan-interval = 5.seconds

    # Duration for blacklisting a peer. Blacklisting reason include: invalid response from peer, response time-out, etc.
    # 0 value is a valid duration and it will disable blacklisting completely (which can be useful when all nodes are
    # are controlled by a single party, eg. private networks)
    # Reduced from 200s to 120s to allow faster retry of peers that may have had transient issues
    blacklist-duration = 120.seconds

    # Duration for high offense blacklisting of a peer. Blacklisting reason include: header validation failure.
    # Reduced from 240 minutes to 60 minutes - still a significant penalty but allows recovery
    critical-blacklist-duration = 60.minutes

    # Retry interval when not having enough peers to start fast-sync
    start-retry-interval = 5.seconds

    # Retry interval for resuming fast sync after all connections to peers were lost
    # Also retry interval in regular sync: for picking blocks batch and retrying requests
    sync-retry-interval = 0.5 seconds

    # Delay between finishing fast sync and starting regular sync
    sync-switch-delay = 0.5 seconds

    # Response time-out from peer during sync. If a peer fails to respond within this limit, it will be blacklisted
    # Increased from 30s to 45s to be more tolerant of network latency and peer load
    peer-response-timeout = 45.seconds

    # Interval for logging syncing status info
    print-status-interval = 30.seconds

    # How often to dump fast-sync status to disk. If the client is restarted, fast-sync will continue from this point
    persist-state-snapshot-interval = 1.minute

    # Maximum concurrent requests when in fast-sync mode
    max-concurrent-requests = 50

    # Requested number of block headers when syncing from other peers
    # Aligned with max-blocks-headers-per-message to request full batches
    # Pushed to safe maximum of 1024 headers for maximum sync throughput
    block-headers-per-request = 1024

    # Requested number of block bodies when syncing from other peers
    # Core Geth limits responses to 2MB, reduced from 100 to 50 to align with
    # max-blocks-bodies-per-message and stay reliably under 2MB soft limit
    block-bodies-per-request = 50

    # Max. number of blocks that are going to be imported in one batch
    blocks-batch-size = 50

    # Requested number of TX receipts when syncing from other peers
    # Aligned with max-receipts-per-message to request full batches
    # Pushed to safe maximum of 1024 receipts for maximum sync throughput
    receipts-per-request = 1024

    # Requested number of MPT nodes when syncing from other peers
    nodes-per-request = 384

    # Minimum number of peers required to start fast-sync (by determining the pivot block)
    min-peers-to-choose-pivot-block = 3

    # Number of additional peers used to determine pivot block during fast-sync
    # Number of peers used to reach consensus = min-peers-to-choose-pivot-block + peers-to-choose-pivot-block-margin
    peers-to-choose-pivot-block-margin = 1

    # Number of peers to fetch the blocks in parallel for execution sync
    peers-to-fetch-from = 5

    # During fast-sync when most up to date block is determined from peers, the actual target block number
    # will be decreased by this value
    pivot-block-offset = 32

    # How often to query peers for new blocks after the top of the chain has been reached
    check-for-new-block-interval = 10.seconds

    # size of the list that keeps track of peers that are failing to provide us with mpt node
    # we switch them to download only blockchain elements
    fastsync-block-chain-only-peers-pool = 100

    # time between 2 consecutive requests to peer when doing fast sync, this is to prevent flagging us as spammer
    fastsync-throttle = 0.1 seconds

    # When we receive a branch that is not rooted in our chain (we don't have a parent for the first header), it means
    # we found a fork. To resolve it, we need to query the same peer for previous headers, to find a common ancestor.
    branch-resolution-request-size = 30

    # threshold for storing non-main-chain blocks in queue.
    # if: current_best_block_number - block_number > max-queued-block-number-behind
    # then: the block will not be queued (such already queued blocks will be removed)
    max-queued-block-number-behind = 1000

    # threshold for storing non-main-chain blocks in queue.
    # if: block_number - current_best_block_number > max-queued-block-number-ahead
    # then: the block will not be queued (such already queued blocks will be removed)
    max-queued-block-number-ahead = 1000

    # Maximum number of blocks, after which block hash from NewBlockHashes packet is considered ancient
    # and peer sending it is blacklisted
    max-new-block-hash-age = 20

    # Maximum number of hashes processed form NewBlockHashes packet
    max-new-hashes = 64

    # This a recovery mechanism for the issue of missing state nodes during blocks execution:
    # off - missing state node will result in an exception
    # on - missing state node will be redownloaded from a peer and block execution will be retried. This can repeat
    #      several times until block execution succeeds
    redownload-missing-state-nodes = on

    # See: https://github.com/ethereum/go-ethereum/pull/1889
    fast-sync-block-validation-k = 100
    fast-sync-block-validation-n = 2048
    fast-sync-block-validation-x = 24

    # Maxium difference beetween our target block and best possible target block (current best known block - offset)
    # This is to ensure that we start downloading our state as close as possible to top of the chain
    max-target-difference = 10

    # Maxium number of failure to update target block, this could happen when target block, or x blocks  after target
    # fail validation. Or when we keep getting old block from the network.
    maximum-target-update-failures = 5

    # Sets max number of blocks that can be stored in queue to import on fetcher side
    # Warning! This setting affects ability to go back in case of branch resolution so it should not be too low
    max-fetcher-queue-size = 1000

    # Expected size fo state sync bloom filter.
    # Current Size of ETC state trie is aroud 150M Nodes, so 200M is set to have some reserve
    # If the number of elements inserted into bloom filter would be significally higher that expected, then number
    # of false positives would rise which would degrade performance of state sync
    state-sync-bloom-filter-size = 200000000

    # Max number of mpt nodes held in memory in state sync, before saving them into database
    # 100k is around 60mb (each key-value pair has around 600bytes)
    state-sync-persist-batch-size = 100000

    # If new pivot block received from network will be less than fast sync current pivot block, the re-try to chose new
    # pivot will be scheduler after this time. Avarage block time in etc/eth is around 15s so after this time, most of
    # network peers should have new best block
    pivot-block-reschedule-interval = 15.seconds

    # If for most network peers, the following condition will be true:
    # (peer.bestKnownBlock - pivot-block-offset) - node.curentPivotBlock > max-pivot-age
    # it fast sync pivot block has become stale and it needs update
    max-pivot-block-age = 96

    # Maximum number of retries performed by fast sync when the master peer sends invalid block headers.
    # On reaching this limit, it will perform branch resolving.
    fast-sync-max-batch-retries = 5

    # Max number of times a pivot block is checked against available best peers before the whole process is restarted.
    max-pivot-block-failures-count = 5
  }

  pruning {
    # Pruning mode that the application will use.
    #
    # - archive:  No pruning is performed
    # - basic:    reference count based pruning
    # - inmemory: reference count inmemory pruning
    #
    # After changing, please delete previous db before starting the client:
    #
    mode = "basic"

    # The amount of block history kept before pruning
    # Note: if fast-sync clients choose target block offset greater than this value, fukuii may not be able to
    # correctly act as a fast-sync server
    history = 64
  }

  node-caching {
    # Maximum number of nodes kept in cache
    # Each key-value pair of nodeHash-Nodencode has around ~600bytes, so cache around ~250Mb equals to 400000 key-value pairs
    max-size = 400000


    # Time after which we flush all data in cache to underlying storage
    # This ensures that in case of quit we lose at most ~5 min of work
    max-hold-time = 5.minutes
  }

  inmemory-pruning-node-caching {
    # Maximum number of nodes kept in cache
    # Cache size rationale:
    # To in memory pruner make sense, cache should have size which enables to hold at
    # least `history * (avg number of state nodes per block)`.
    # Current estmates are that at the top of eth chain each block carries 10k states nodes.
    # so at the moment: 64 (current history) * 10000 = 640000
    # This is lower bound, but to be prepared for future or accomodate for larger block it is worth to have cache
    # a little bit larger.
    # Current number = 2 * lowerBound
    # Cache size in mb:
    # Each key-value pair of nodeHash-Nodencoded has around ~700bytes -
    # 600b - encoded mpt node (this is upper bound, as many nodes like leaf nodes, are less than 150)
    # 4b   - Int, number of references
    # 24b  - BigIng, blocknumber, value taken from visual vm
    # 32b  - Cache key, hash node encoded
    # All sums to 660b, but there is always overhead for java objects.
    # Taking that in to account cache of 1280000 object will require at most: 896mb. (but ussually much less)
    #
    max-size = 1280000


    # Time after which we flush all data in cache to underlying storage
    # This ensures that in case of quit we lose at most ~60 min of work
    max-hold-time = 60.minutes
  }

  db {
    rocksdb {
      # RocksDB data directory
      path = ${fukuii.datadir}"/rocksdb/"

      # Create DB data directory if it's missing
      create-if-missing = true

      # Should the DB raise an error as soon as it detects an internal corruption
      paranoid-checks = true

      # This ensures that only one thread will be occupied
      max-threads = 1

      # This ensures that only 32 open files can be accessed at once
      max-open-files = 32

      # Force checksum verification of all data that is read from the file system on behalf of a particular read
      verify-checksums = true

      # In this mode, size target of levels are changed dynamically based on size of the last level
      # https://rocksdb.org/blog/2015/07/23/dynamic-level.html
      level-compaction-dynamic-level-bytes = true

      # Approximate size of user data packed per block (16 * 1024)
      block-size = 16384

      # Amount of cache in bytes that will be used by RocksDB (32 * 1024 * 1024)
      block-cache-size = 33554432
    }

    # Define which database to use [rocksdb]
    data-source = "rocksdb"

    # Run database checks every 10 mins and shut down when an inconsistency is found
    periodic-consistency-check = false
  }

  filter {
    # Time at which a filter remains valid
    filter-timeout = 10.minutes

    filter-manager-query-timeout = 3.minutes
  }

  vm {
    # internal | external
    mode = "internal"

    external {
      # possible values are:
      # - iele: runs a binary provided at `executable-path` with `port` and `host` as arguments (`./executable-path $port $host`)
      # - kevm: runs a binary provided at `executable-path` with `port` and `host` as arguments (`./executable-path $port $host`)
      # - fukuii: if `executable-path` is provided, it will run the binary with `port` and `host` as arguments
      #           otherwise fukuii VM will be run in the same process, but acting as an external VM (listening at `host` and `port`)
      # - none: doesn't run anything, expect the VM to be started by other means
      vm-type = "fukuii"

      # path to the executable - optional depending on the `vm-type` setting
      executable-path = "./bin/fukuii-vm"

      host = "127.0.0.1"
      port = 8888
    }
  }

  async {
    ask-timeout = 60.seconds

    dispatchers {
      block-forger {
        type = Dispatcher
        executor = "fork-join-executor"

        fork-join-executor {
          parallelism-min = 2
          parallelism-factor = 2.0
          parallelism-max = 8

          task-peeking-mode = "FIFO"
        }

        throughput = 5
      }
    }
  }
}

pekko {
  loggers = ["org.apache.pekko.event.slf4j.Slf4jLogger"]

  # Set to DEBUG for troubleshooting network sync issues
  loglevel = "DEBUG"
  loglevel = ${?PEKKO_LOGLEVEL}

  logging-filter = "org.apache.pekko.event.slf4j.Slf4jLoggingFilter"
  logger-startup-timeout = 30s
  log-dead-letters-during-shutdown = off
  log-dead-letters = 5

  coordinated-shutdown.phases {
    actor-system-terminate {
      timeout = 15 s
    }
  }
}

include "metrics.conf"

# DEBUGGING SETTING
# Uncomment to enable non-standard mailbox for all actors
# Mailbox will start logging actor path, when actor mailbox size will be bigger than `size-limit`
# Useful when looking for memory leaks caused by unbounded mailboxes
#
# pekko.actor.default-mailbox {
#  mailbox-type = com.chipprbots.ethereum.logger.LoggingMailboxType
#  size-limit = 10000
# }

# Bounded mailbox configured for SignedTransactionsFilterActor.
# Actor is resposible for calculating sender for signed transaction which is heavy operation, and when there are many
# peers it can easily overflow
bounded-mailbox {
  mailbox-type = "org.apache.pekko.dispatch.NonBlockingBoundedMailbox"
  mailbox-capacity = 50000
}

pekko.actor.mailbox.requirements {
  "org.apache.pekko.dispatch.BoundedMessageQueueSemantics" = bounded-mailbox
}

# separate threadpool for concurrent header validation
validation-context {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 4
  }
  throughput = 1
}

logging {
  # Flag used to switch logs to the JSON format
  json-output = false

  # Logs directory
  logs-dir = ${fukuii.datadir}"/logs"

  # Logs filename
  logs-file = "fukuii"

  # Logs level - defaults to ERROR for minimal production logging
  # Only actionable activities (errors) and high-level lifecycle events are logged by default
  # Set to INFO for more detailed status messages, or DEBUG for troubleshooting
  # NB. be aware you might want to adjust akka.loglevel as well if set to DEBUG
  # Override via environment: -Dlogging.logs-level=DEBUG or FUKUII_LOGS_LEVEL=DEBUG
  logs-level = "ERROR"
}
